\chapter{引言}\label{chap:introduction}

\section{研究背景}
从图像中恢复三维几何是计算机视觉需要解决的根本问题之一，针对不同物体的几何重建，在工业和商业上有诸多应用。随着虚拟现实(Virtual Reality, VR)和增强现实(Augmented Reality, AR)硬件设备的工业流程进步，诸如线上购物、虚拟看房、场景融合等应用对物体的几何恢复有很紧迫的需求，而针对人的几何重建在虚拟试衣、数字人以及全息视频等应用中属于关键技术。由此可见几何重建的好坏将对未来的教育、电商、沟通等诸多方面有着相当大的影响。

工业界当前用于几何建模的生产工具主要以面片和经典参数化曲线作为几何的表达形式，这些传统的几何表达方式有着利用手工编辑、存储规范以及适配渲染贴图等流程的优点。但是随着模型几何生成的自动化程度提高，人工编辑的工作量在下降，只需对算法生成的结果进行部分修正，而传统的面片表达形式对瑕疵过于敏感，少许几何上的瑕疵可能极大影响观赏体验，使得算法自动生成模型的优势无法体现。另一方面随着深度学习在三维重建方面的应用，越来越多的研究工作将人和物体的重建建模成一个连续可导的过程，而面片表达本质上为离散图结构，显然不是与神经网络结合的最佳选择。不仅如此，近年来涌现出的诸多可导渲染器对材质和贴图的表示方法也提供了新的可能，这同样使得以面片和参数化曲线表达的建模方式不再是必须的。

三维几何除开本身作为独立的研究目标，还与其余诸如渲染、自由视点、材质光照估计等领域有着紧密联系。所有与3D视觉相关的问题，都绕不开选取适当的几何表达作为中间件这一过程。在光场渲染中常常需要较为粗糙的几何模型作为先验，使得渲染出来的图片或视频具备一定的视角依赖性，以提高真实感。在自由视点插值中，同样需要几何来提供图片之间的像素对应关系。相比于得到高质量的几何模型，在这些问题中更加关注最终呈现出来的图片和视频的质量，而允许作为中间件的几何模型有一定的瑕疵，这使得对几何表达形式的选取变得更加微妙。

近年来深度学习与3D视觉结合的方法在诸如单视角恢复几何(Single View Reconstruction, SVR)、多视角重建(Multi-View Stereo, MVS)以及神经渲染(Neural Rendering, NR)等诸多方面取得了超越之前传统方法的结果\citep{jiang2020sdfdiff, mildenhall2020nerf, park2019cvpr, saito2019pifu, meshcheder2019cvpr}。这些工作开始探索将包括传统的面片、体积离散以及隐函数等形式的几何表达融合进深度学习的框架中，而在这些表达中，相比于以面片为主的显式表达，隐式几何表达明显更适合深度学习框架下的3D视觉任务。


\section{研究现状}
围绕几何重建的工作包括关注几何本身的和应用几何信息作为中间件来达到渲染以及视角生成等目的的工作，而这些问题本身的输入来源又多种多样。本文讨论的工作局限于从二维的图像得到三维信息，以下按照应用目的分类进行几个方面的研究现状综述。

\subsection{多视角图片的重建}
在人物重建和物品重建方面，多视角重建用来生成完整的带贴图的三维模型，采集系统的搭建非常复杂，常常需要多达几十甚至上百的相机进行同步拍摄。早期的工作包括从轮廓信息重建\citep{vlastic2008, furukawa2006, matusik2000ibvh}，将2D图片中提取的轮廓投影成空间中的可见视锥区域并对所有的可见区域求交集“切”出最终的模型。这种方法在图片数量足够的情况下能获得不错的几何，但是无法处理非凸的情形。精细的几何和纹理信息就要依赖于多视角立体匹配约束\citep{furukawa2010, starck2007, zitnick2004}，这些方法利用多视角图片中特征点的匹配，计算出稀疏点云或者分层的深度图来作为几何表达，在此基础上生成密集点云以及进行补全和去噪，再将点云用泊松重建\citep{kazhdan2013, kazhdan2006}等方式恢复成面片。多视角设定下通过可控地改变光照条件拍摄多张照片也能恢复出几何\citep{wu2012, vlasic2009},这类方法通过恢复像素级的法向信息来估计几何，精度较高，但是对光源搭建和拍照环境的要求较高，对于非朗伯表面的物体诸如金属等反光材质的效果无法保证。

在多视角的设定下用深度学习框架来生成3D表面的工作集中在将传统的特征提取和匹配框架换成完全可导的连续过程。三维网格是这类方法中一种典型的表达几何的形式，\citet{ji2017iccv}通过计算离散网格的代价函数，并据此对每个格点指定对应的视角图片，并用3D卷积网络来对网格是否属于表面进行回归。\citet{kar2017neurips}通过可导的投影和反投影算子，将图片中的特征融合到三维网格中，以此为依据对网格进行分类。另一类工作试图利用深度学习对数据的学习能力，来降低所需图片和视角的数量，以达到稀疏多视角设定下高质量的三维重建，这有利于减轻多视角拍摄系统的搭建，节省大量成本，意义非常重大。通过使用轮廓信息得到粗糙的几何模型再利用颜色信息和网络去细调，来得到高精度的模型\citep{Gilbert_2018_ECCV,huang2018eccv}，本质上能减少输入的视角数量，是由于训练时使用了高精度模型作为监督。所有这些基于三维网格的几何表达方式都难以避免内存开销巨大的弊病，因此无法适用于大型场景，同时只能在低分辨率网格上表达粗滤的几何结构。\citet{yao2018mvsnet}使用深度图的形式表达几何，减少了内存开销，最终的重建结果由深度图反投影的点云表示，但是在网络中还是使用了离散网格表达相关的度量张量。

对存储依赖真正减少的契机出现在\citet{meshcheder2019cvpr}提出用多层感知机(Multi-Layer Perceptron, MLP)来表达几何，通过一个MLP网络对空间中任意点在表面内部还是外部进行分类，而最终的表面则通过两类区域的边界构成，数学上这就是图形学领域早就存在的使用隐函数来表达几何。仅仅预测点在表面的内部还是外部形式无法应用隐函数更多的几何和代数性质，一些工作干脆将有符号距离场(Signed Distance Function, SDF)引入到深度学习框架中\citep{jiang2020sdfdiff, park2019cvpr}，同样使用MLP表示。SDF是一个表达空间中任意点到表面距离的函数，同时距离的符号表达在面的内部还是外部，由于其在表面附近拥有关于法向的良好性质，有助于提升表面的重建细节。\citet{yariv2020multiview}使用SDF表达结合由向量表达的材质信息，使用球追踪(Sphere Tracing)方法进行可导渲染的方式来同时分离并重建几何和材质信息，取得了很好的效果。

\subsection{单视角图片的重建}
单视角的重建根据场景不同有很大区别，若物体为静态且基本上符合朗伯表面的假设，则可以移动相机等效于在多个视角同时拍摄，把问题转化为多视角重建。真正意义上的单视角重建实际上针对的是单张图片或者是视频，而根据相机类型的不同，又可以进一步加以分类。\citet{newcombe2011kfuse}提出了以深度图流为输入的实时重建算法，该方法拉开了以颜色加深度为输入进行实时重建的序幕，同时确立了以配准和融合交替进行为框架的算法框架，其中配准过程是将模型对齐到当前数据，融合是以截断的SDF为几何表达进行模型更新的过程。这里展示出了SDF另一条突出的性质：与传统面片表达不易修改拓扑关系的弱点相比，SDF能够通过连续的迭代过程，平滑地对几何进行变换，包括拓扑变化。同时SDF的隐式几何能迅速地转化为面片\citep{lorensen1987}。然而仅仅是刚性场景的重建只能表达静态的情况，单相机模型中往往要对时序数据进行处理，动态场景变得非常常见，显而易见以采集人体为例，要求人在几十秒至数分钟的采集时间内纹丝不动过于苛刻。\citet{zollhofer2014}提出了对一般性物体的非刚性实时重建算法，将非刚性的迭代最近点算法(Non-rigid ICP)与融合过程结合在一起。自此开始的非刚性实时重建算法在配准的计算过程和目标函数设计上有诸多变化\citep{slavcheva2017, dou2016, innmann2016volume, newcombe2015}，有的基于三维网格的形变，有的基于面片简化后的拓扑图进行计算，但是融合过程始终还是用截断SDF表达。但是基于深度信息的重建算法对实现要求很高，算法复杂，鲁棒性差，同时考虑到实际应用中以手机相机最为普及，而当时手机摄像头还未配置深度相机，这使得研究者们开始思考绕开深度信息以及配准加融合的算法框架，仅考虑彩色图片和视频的输入。

基于彩色图片或者视频进行重建是非常病态的问题，约束条件太少，这就使得求解必须依赖于足够充分的先验知识或者假设。由于基于人体的参数化模型研究基础比较充分\citep{loper2015smpl, anguelov2005scape}，很多工作将问题转化为从图片估计模型参数。这类方法中，图片的轮廓信息、关节点信息\citep{cao2019openpose, fang2017rmpe}以及其他一些手工标注的信息，常常被用来初始化拟合参数\citep{zhou2010, guan2009}。近年出现了设计深度学习框架直接端到端去预测模板参数的方法\citep{Pavlakos_2018_CVPR, kanazawa2018}，提高了对一些极端遮挡情况的鲁棒性，同时能够融入诸如人体分割信息这样本身就适合深度学习的子任务联合训练\citep{omran2018nbf}，获得更好的效果。\citet{alldieck2018video}对视频输入通过优化多帧共同的形状参数和逐帧的姿态参数，求解时间序列上的连续模板参数，并通过轮廓进行细节的调优。但是参数化模型本身仅仅是表达了一个赤裸的人体，对于穿紧身衣服的情况还能适用，但是对于比较宽松的衣服、裙子以及头发或者是手上提了东西的情景就无法表达了。因此研究者们开始回归到无模板的方法，\citet{varol2018}提出了直接学习一个固定分辨率的三维网格的方法，但是由于网格消耗内存的缘故，几何的细节完全丢失掉了。与多视角情形相似，隐函数的引入解决了存储消耗的问题。\citet{saito2019pifu}提出了将隐函数形式与像素对齐的方法，通过将特征张量投影到像素空间来建立几何与图片像素之间的数学模型，并通过大量高精模型与图片构成的数据对进行学习，同时这个框架还能推广到多视角的情形。该工作不久又通过引入法向图的预测并由此对几何模型进行细调，而得到了精度和纹理更加好的结果\citep{saito2020pifuhd}。\citet{Chibane_2020_CVPR}同样采用隐函数的形式，与前面不同的在于不仅考虑一个点或者像素，而是对包含一个点的局部空间进行整体考虑，将这个局部空间离散成一个不大的网格作为输入，文章提出的方法适用于多项任务。

\subsection{神经网络表达的视角生成}
视角生成本身属于一个独立的研究领域，很多基于图像渲染的工作专门绕开几何信息而直接试图合成新的视角图片，与本文关注几何的根本出发点无关，在此并不综述这方面的文章。另一些视角合成的工作则是将几何表达作为中间件，通过选取特定的几何表达形式，来更好地提取和融合图片中的3D信息，最后再将目标锁定在合成的图片质量上，而作为中间件的几何质量并不在直接的考量范围之内。但是即便是带几何的视角合成工作数量也很庞大，本文在此仅关注结合了深度学习以及和神经渲染相关的视角生成工作。

卷积神经网络最初被引进视角合成的任务时并没有考虑几何表达，而是仅仅利用卷积的结构去拟合数据集来完成新视角合成\citep{TDB16a}以及帧间插值等\citep{niklaus2017}。\citet{jaderberg2015}提出了往卷积网络里面接入可导模块进行几何变换，这才使得在网络中加入几何相关的表达成为可能。\citet{flynn2016}使用深度图来融合网络预测出的不同深度的图片，\citet{kalantari2016}使用视差来表达几何对光场数据做新视角的合成，\citet{zhou2016view}使用光流作为中间件来预测输入图片到新视角图片的像素之间的关系，\citet{tvsn_cvpr2017}同样使用光流作为几何表达，但是为了解决遮挡区域的补全问题，又引入了对抗网络通过学习数据的分布来猜出遮挡区域更真实的像素信息。不难发现，这些方法里面的几何表达方法全都是隐式的表达，与3D的隐函数表达不同，这些几何表达方式已经被离散到2D网格上并于像素对齐，能够更好地利用卷积网络感知局部信息的优势。但是卷积网络基本上只能对于图片做整体的卷积操作，生成的图像往往有模糊和振铃等瑕疵，细节也很难恢复。

最近的工作开始从光线的角度重新考虑视角合成，这类工作通常用网络来表达隐式几何，输入空点中点的位置和光线方向，网络则输出诸如SDF函数值或者其他的隐函数值，并以此逐根光线地渲染出整幅图。\citet{sitzmann2019srns}使用LSTM来对场景中每个空间中的连续点进行编码，提却特征并模拟光线追踪的过程来渲染图片。\citet{mildenhall2020nerf}借鉴了烟雾的几何表达，使用一个MLP来预测空间点的密度，同时预测与视角关联的光线上各点的颜色值，并用采样离散点结合体渲染的方式渲染出最终的颜色。\citet{oechsle2020}使用网络从输入图片中表达场景的表面光场，并同时恢复光照信息，使得可以实现带重打光功能的新视角合成。


\section{本文研究的问题}
从研究现状的分析中可以发现，基于图片的三维重建技术本身以及以几何作为中间件的渲染和图片生成领域，都已经和深度学习技术框架广泛结合。隐式几何表达不仅具备可导性，其针对具体应用的表达形式对存储需求的调整灵活度还很高。因此本文的注意力放在隐式几何表达与深度学习框架的结合上，结合视角插值和点云重建两个具体的应用进行研究。

对于静态场景的视角插值，传统方法的重点与立体匹配类似，主要分为寻找两张图之间的密集特征匹配和插值融合两步\citep{seitz1996}。\citet{Ji_2017_CVPR}将传统框架变成了基于深度学习的可导框架，但是遮挡变化、大相机基线下的插值以及多张图之间的连续变化这些问题并没有解决。本文针对这些缺陷，设计了新的三相机视角插值框架。包含了新的预矫正模型，对多张图之间的连续插值有逼近二阶连续的轨迹，同时引入了基于视差的隐式几何表达作为中间件，可以导出图片之间的密集特征，最终通过带有效区域加权的可导融合方法合成新视角的图片。通过在一般物体的数据集\citep{shapenet2015}和人体数据集\citep{varol2017}上进行了训练和与其他方法的对比，证明本文的新方法对于遮挡造成的歧义能够比较好的消除，合成高质量的连续中间图片。新的框架还支持通过连续插值产生360度的“子弹时间”效果，而这一切背后的支撑主要在于隐式几何的选择，和据此进行的误差函数的设计。

对于点云重建的任务，目标主要是从粗糙的点云恢复出显式几何，这些输入点云的来源有多种可能，往往是带噪声和空洞等几何缺陷、同时分辨率可能还偏低的扫描数据。经典传统方法当属泊松重建\citep{kazhdan2013, kazhdan2006}，对点云的重建仅仅利用局部数据的位置和法向信息进行，相当于无任何先验的空间上的插值。本文提出的方法基于深度学习框架，将离散的点云转化为隐函数的形式，并在此基础上进行补全和去噪，最终提取显式几何。对于点云本方法首先通过可导渲染的方式将点云编码成多视角的图像形式，用MLP代表隐函数，输入空间点的位置信息和从点云中提取的特征，预测隐函数的输出值。通过在两个不同的人体数据上的训练与测试\citep{muller2009}，本方法能够同时利用点云数据本身的信息以及从高精度数据中学习到的信息，生成去噪和补全后的模型，并能够保留更多的几何细节，说明了隐式几何表达结合深度学习在该任务上的优势。

\section{本文的组织结构}
本文工作包含两个基于隐式几何在具体任务上的应用，本文余下内容将分别阐述两个任务的方法以及实验结论。本文的组织结构如下：

第二章介绍环形三相机视角变换的方法。主要包含算法模块中的各个部分的原理介绍与讨论，即环形矫正算法、编解码器神经网络的设计、可见区域的预测以及视角融合生成模块以及目标函数的设计。

第三章介绍环形三相机视角变换方法的实现与实验结果，以及与其他基于深度学习的视角生成算法的比较。

第四章介绍基于隐函数的点云重建方法。包括从点云可导渲染到多个视角的过程，二维上的特征提取与补全，隐函数模块的预测与目标函数的设计方案。

第五章介绍基于隐函数的点云重建实现方式、训练数据的准备和实验结果，并与传统泊松重建结果的比较。

第六章对工作进行总结和展望，对隐式几何在深度学习框架下对重建和渲染的效果进行总结，并指出未来继续改进甚至在实际生产中应用的前景。